import ollama
import subprocess
from typing import Any, Dict, Tuple, Optional
from .config import HF_TOKEN
from huggingface_hub import snapshot_download, HfApi
from .expander_registry import register_model, suggested_local_dir, get_adapter

from transformers import (
    AutoModelForCausalLM,
    AutoModelForSequenceClassification,
    AutoModelForTokenClassification,
    AutoModelForSeq2SeqLM,
    AutoConfig, AutoTokenizer, AutoModel
)

# ä¸€äº›å¸¸è¦‹ encoderâ€“decoder å®¶æ—ï¼ˆé‡åˆ°é€™äº›ï¼Œå„ªå…ˆèµ° Seq2Seqï¼‰
ENCDEC_FAMILIES = {
    "t5", "mt5", "bart", "mbart", "marian", "pegasus", "prophetnet"
}

# æŸäº› pipeline_tag å°æ‡‰çš„é è¨­ Loader
TASK_TO_LOADER = {
    "text-generation": "causal",          # ä½†è‹¥ cfg æ˜¯ enc-dec â†’ è‡ªå‹•åˆ‡ seq2seq
    "summarization": "seq2seq",
    "translation": "seq2seq",
    "text-classification": "seqcls",
    "token-classification": "tokcls",
    "feature-extraction": "auto",
    # å…¶é¤˜ä»»å‹™ï¼ˆasrã€image-to-textã€text-to-imageâ€¦ï¼‰äº¤çµ¦ä½ çš„ Adapterï¼Œä¸ç”¨é€™å€‹ loader
}

def _pick_loader_kind(pipeline_tag: Optional[str], cfg) -> str:
    """
    å›žå‚³ 'causal' / 'seq2seq' / 'seqcls' / 'tokcls' / 'auto'
    """
    # 1) å…ˆçœ‹ pipeline_tag çš„ç›´è¦ºå°æ‡‰
    if pipeline_tag in TASK_TO_LOADER:
        kind = TASK_TO_LOADER[pipeline_tag]
    else:
        kind = "causal"  # é è¨­å…ˆè©¦ decoder-onlyï¼ˆæœ€å¸¸è¦‹ï¼‰

    # 2) è‹¥æ¨¡åž‹æœ¬è³ªæ˜¯ encoderâ€“decoderï¼Œå¼·åˆ¶æ”¹æˆ seq2seq
    is_encdec = getattr(cfg, "is_encoder_decoder", False)
    model_type = (getattr(cfg, "model_type", "") or "").lower()
    if is_encdec or model_type in ENCDEC_FAMILIES:
        # åªæœ‰åˆ†é¡žä»»å‹™é™¤å¤–ï¼ˆå°‘æ•¸ enc-dec ä¹Ÿèƒ½åˆ†é¡žï¼Œä½†ä½ é€™è£¡ä¸»è¦è¼‰ç”Ÿæˆ/ç¿»è­¯ï¼‰
        if kind not in ("seqcls", "tokcls"):
            kind = "seq2seq"
    return kind

def load_model(model_id: str, task_override: Optional[str] = None) -> Tuple[Optional[object], Optional[object]]:
    """
    - model_id: HF repo_id å­—ä¸²ï¼ˆæ³¨æ„ï¼šä¸è¦å‚³ dict/tupleï¼‰
    - task_override: è‹¥ä½ å·²çŸ¥å¤–å±¤ä»»å‹™ï¼ˆå¦‚ 'summarization'ï¼‰ï¼Œå¯æ˜Žç¢ºæŒ‡å®šï¼›å¦å‰‡ç”¨ HF æ¨™ç±¤ + cfg åˆ¤æ–·
    å›žå‚³: (model, tokenizer) æˆ– (None, None)
    """
    if not isinstance(model_id, str):
        print("â— Transformers è¼‰å…¥å¤±æ•—ï¼šmodel_id ä¸æ˜¯å­—ä¸²ï¼ˆè«‹ç¢ºèª UI å‚³å…¥çš„æ˜¯ repo_id å­—ä¸²ï¼‰")
        return None, None

    api = HfApi()
    try:
        info = api.model_info(model_id, token=HF_TOKEN)
    except Exception as e:
        print(f"â— è®€å–æ¨¡åž‹è³‡è¨Šå¤±æ•—ï¼š{e}")
        return None, None

    # è®€ tokenizer & configï¼ˆå„ªå…ˆç”¨ fastï¼›å…è¨± remote codeï¼‰
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN, use_fast=True, trust_remote_code=True)
    except Exception as e:
        print(f"âš ï¸ Tokenizer è¼‰å…¥è­¦å‘Šï¼š{e}ï¼Œæ”¹ç”¨éž fast å˜—è©¦")
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN, use_fast=False, trust_remote_code=True)
        except Exception as e2:
            print(f"â— Tokenizer è¼‰å…¥å¤±æ•—ï¼š{e2}")
            tokenizer = None  # æŸäº›ä»»å‹™ï¼ˆfeature-extractionï¼‰å¯å®¹å¿ç„¡ tokenizer

    try:
        cfg = AutoConfig.from_pretrained(model_id, trust_remote_code=True)
    except Exception as e:
        print(f"â— è®€å– AutoConfig å¤±æ•—ï¼š{e}")
        return None, None

    # å…ˆæ±ºå®šè¦ç”¨å“ªç¨® Loader
    pipe_tag = (task_override or info.pipeline_tag or "").strip()
    loader_kind = _pick_loader_kind(pipe_tag, cfg)

    try:
        if loader_kind == "seq2seq":
            model = AutoModelForSeq2SeqLM.from_pretrained(model_id, token=HF_TOKEN, trust_remote_code=True)
        elif loader_kind == "seqcls":
            model = AutoModelForSequenceClassification.from_pretrained(model_id, token=HF_TOKEN, trust_remote_code=True)
        elif loader_kind == "tokcls":
            model = AutoModelForTokenClassification.from_pretrained(model_id, token=HF_TOKEN, trust_remote_code=True)
        elif loader_kind == "auto":
            model = AutoModel.from_pretrained(model_id, token=HF_TOKEN, trust_remote_code=True)
        else:
            # 'causal'
            # è‹¥ä¸å°å¿ƒé‡åˆ° enc-dec ä»èµ°åˆ°é€™è£¡ï¼Œå†å…œåº•ä¸€æ¬¡
            if getattr(cfg, "is_encoder_decoder", False):
                model = AutoModelForSeq2SeqLM.from_pretrained(model_id, token=HF_TOKEN, trust_remote_code=True)
            else:
                model = AutoModelForCausalLM.from_pretrained(model_id, token=HF_TOKEN, trust_remote_code=True)

        model.eval()
        return model, tokenizer

    except Exception as e:
        msg = str(e)
        if "T5Config" in msg or "is_encoder_decoder" in msg or "BART" in msg:
            msg += "\nðŸ’¡ æç¤ºï¼šé€™æ˜¯ encoderâ€“decoder æ¨¡åž‹ï¼Œè«‹ç”¨ AutoModelForSeq2SeqLM è¼‰å…¥ã€‚"
        print(f"â— Transformers è¼‰å…¥å¤±æ•—ï¼š{msg}")
        return None, None


# def load_model(model_id):
#     api = HfApi()
#     info = api.model_info(model_id, token=HF_TOKEN)

#     try:
#         tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN)

#         task = info.pipeline_tag or ""
#         if task == "text-generation":
#             model = AutoModelForCausalLM.from_pretrained(model_id, token=HF_TOKEN)
#         elif task == "text-classification":
#             model = AutoModelForSequenceClassification.from_pretrained(model_id, token=HF_TOKEN)
#         elif task == "token-classification":
#             model = AutoModelForTokenClassification.from_pretrained(model_id, token=HF_TOKEN)
#         elif task in ("translation", "summarization"):
#             model = AutoModelForSeq2SeqLM.from_pretrained(model_id, token=HF_TOKEN)
#         else:
#             print(f"âš ï¸ ä¸æ”¯æ´çš„ pipeline_tagï¼š{task}ï¼Œè«‹é¸æ“‡å…¶ä»–æ¨¡åž‹")
#             return None, None

#         return model, tokenizer

#     except Exception as e:
#         print(f"â— Transformers è¼‰å…¥å¤±æ•—ï¼š{e}")
#         return None, None

# --- ä¸‹é¢æ˜¯æ–°å¢žçš„å·¥å…· ---

def ensure_local_copy(repo_id: str) -> str:
    """
    ä¸‹è¼‰æ•´å€‹ repo åˆ° project/expanded_models/<å®‰å…¨å>ï¼Œå›žå‚³æœ¬åœ°è·¯å¾‘ã€‚
    """
    local_dir = suggested_local_dir(repo_id)  # ä¾‹å¦‚ project/expanded_models/org__name
    snapshot_download(
        repo_id=repo_id,
        local_dir=local_dir,
        local_dir_use_symlinks=False,  # å¯«å¯¦é«”æª”ï¼Œé¿å… symlink è¸©é›·
        ignore_patterns=[]              # ä¹Ÿå¯ä¿ç•™ç©ºï¼Œå®Œæ•´ä¸‹è¼‰
    )
    return local_dir

def register_transformers(repo_id: str, task: str, local_dir: str):
    register_model(
        repo_id=repo_id,
        backend="transformers",
        task=task or "",
        local_path=local_dir,
        notes=""
    )

def load_model_by_task(model_id: str, task: str, **kwargs) -> Any:
    adapter = get_adapter(task)
    if adapter is None:
        raise ValueError(f"No adapter for task: {task}")
    adapter.prepare(model_id, task=task, **kwargs)
    return adapter.load(model_id, **kwargs)